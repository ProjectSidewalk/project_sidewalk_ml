{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "#import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "\n",
    "from TwoFileFolder import TwoFileFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/mnt/c/Users/gweld/sidewalk/sidewalk_ml/baby_ds/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {x:TwoFileFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "                  for x in ['train', 'test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We've got things set up like in train.py\n",
    "\n",
    "now let's play around with a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = image_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['missing_ramp', 'null_crop', 'obstruction', 'ramp', 'sfc_problem']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_img, target = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(example_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7137, -0.7822, -0.7479,  ..., -0.5596, -0.6109, -0.7137],\n",
       "         [-0.7308, -0.7993, -0.7137,  ..., -0.7137, -0.7308, -0.7479],\n",
       "         [-0.7137, -0.7822, -0.6794,  ..., -0.7993, -0.8335, -0.8507],\n",
       "         ...,\n",
       "         [-1.2103, -1.2274, -1.2103,  ..., -1.0904, -1.0904, -1.0904],\n",
       "         [-1.3302, -1.2617, -1.1589,  ..., -1.0733, -1.0733, -1.1075],\n",
       "         [-1.3302, -1.2788, -1.1760,  ..., -1.0562, -1.0733, -1.1075]],\n",
       "\n",
       "        [[-0.7752, -0.8452, -0.7752,  ..., -0.2850, -0.3025, -0.3550],\n",
       "         [-0.7752, -0.8277, -0.7577,  ..., -0.4251, -0.4426, -0.4601],\n",
       "         [-0.7402, -0.8102, -0.7227,  ..., -0.5301, -0.5651, -0.5826],\n",
       "         ...,\n",
       "         [-0.7752, -0.7927, -0.7752,  ..., -0.7577, -0.7577, -0.7577],\n",
       "         [-0.8978, -0.8277, -0.7402,  ..., -0.7577, -0.7752, -0.8102],\n",
       "         [-0.9153, -0.8803, -0.7752,  ..., -0.7752, -0.7927, -0.8102]],\n",
       "\n",
       "        [[-0.7238, -0.7936, -0.7413,  ...,  0.1999,  0.1651,  0.0953],\n",
       "         [-0.7413, -0.7936, -0.7064,  ...,  0.0256,  0.0431,  0.0431],\n",
       "         [-0.7064, -0.7761, -0.6715,  ..., -0.0964, -0.0790, -0.0615],\n",
       "         ...,\n",
       "         [-0.2532, -0.2707, -0.2532,  ..., -0.2532, -0.2532, -0.2532],\n",
       "         [-0.3753, -0.3055, -0.2010,  ..., -0.2532, -0.2707, -0.2881],\n",
       "         [-0.3927, -0.3578, -0.2532,  ..., -0.2532, -0.2707, -0.3055]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_img.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so the image returned by our transform is a 3x224x224 tensor... let's see how we can add on to that a couple extra features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with experimenting with the json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_meta_file = train_dataset.samples[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(example_meta_file) as metafile:\n",
    "    meta = json.load(metafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'crop size': 1067.9562786956378,\n",
       " u'crop_x': 1890.7857638889081,\n",
       " u'crop_y': 4428.0,\n",
       " u'pano id': u'__BdOPEuXx_6Nwts-Ee1tw',\n",
       " u'pano yaw': -111.12658691406199,\n",
       " u'sv_x': 6000.0,\n",
       " u'sv_y': -1100.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def meta_to_tensor(path_to_meta):\n",
    "    ''' used by getitem to load the meta into a tensor'''\n",
    "    with open(path_to_meta) as metafile:\n",
    "        meta_dict = json.load(metafile)\n",
    "        \n",
    "        features = []\n",
    "        # crop size as proxy for depth\n",
    "        # hacky approximate normilization\n",
    "        features.append( meta_dict[u'crop size']/1000 )\n",
    "        \n",
    "        # pano yaw degree\n",
    "        features.append( np.sin(np.deg2rad(meta_dict[u'pano yaw'])) )\n",
    "        features.append( np.cos(np.deg2rad(meta_dict[u'pano yaw'])) )\n",
    "        \n",
    "        # sv_x converted to degree\n",
    "        horiz_degree = (meta_dict[u'sv_x'] / 13312) * 360\n",
    "        features.append( np.sin(np.deg2rad( horiz_degree )) )\n",
    "        features.append( np.cos(np.deg2rad( horiz_degree )) )\n",
    "        \n",
    "        # sv_y converted to degree\n",
    "        vert_degree = (meta_dict[u'sv_y'] / 3328) * 360\n",
    "        features.append( np.sin(np.deg2rad( vert_degree )) )\n",
    "        features.append( np.cos(np.deg2rad( vert_degree )) )\n",
    "        \n",
    "        return torch.Tensor( features )\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.0680, -0.9328, -0.3604,  0.3047, -0.9524, -0.8747, -0.4847])\n",
      "torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "example_meta = meta_to_tensor(example_meta_file)\n",
    "print example_meta\n",
    "print example_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_view = example_img.view(3*224*224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150528])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_view.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "both = torch.cat((new_view, example_meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150535])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "both.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150535"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example_meta) + 150528"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "both[:150528].view((3,224,224)).allclose(example_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "both[150528:].allclose(example_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_both = torch.cat((both.view((1,150535)),both.view((1,150535)),both.view((1,150535)),both.view((1,150535))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 150535])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_both.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_img = torch.cat((example_img.view((1,3,224,224)),example_img.view((1,3,224,224)),example_img.view((1,3,224,224)),example_img.view((1,3,224,224))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 224, 224])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7137, -0.7822, -0.7479,  ..., -0.5596, -0.6109, -0.7137],\n",
       "         [-0.7308, -0.7993, -0.7137,  ..., -0.7137, -0.7308, -0.7479],\n",
       "         [-0.7137, -0.7822, -0.6794,  ..., -0.7993, -0.8335, -0.8507],\n",
       "         ...,\n",
       "         [-1.2103, -1.2274, -1.2103,  ..., -1.0904, -1.0904, -1.0904],\n",
       "         [-1.3302, -1.2617, -1.1589,  ..., -1.0733, -1.0733, -1.1075],\n",
       "         [-1.3302, -1.2788, -1.1760,  ..., -1.0562, -1.0733, -1.1075]],\n",
       "\n",
       "        [[-0.7752, -0.8452, -0.7752,  ..., -0.2850, -0.3025, -0.3550],\n",
       "         [-0.7752, -0.8277, -0.7577,  ..., -0.4251, -0.4426, -0.4601],\n",
       "         [-0.7402, -0.8102, -0.7227,  ..., -0.5301, -0.5651, -0.5826],\n",
       "         ...,\n",
       "         [-0.7752, -0.7927, -0.7752,  ..., -0.7577, -0.7577, -0.7577],\n",
       "         [-0.8978, -0.8277, -0.7402,  ..., -0.7577, -0.7752, -0.8102],\n",
       "         [-0.9153, -0.8803, -0.7752,  ..., -0.7752, -0.7927, -0.8102]],\n",
       "\n",
       "        [[-0.7238, -0.7936, -0.7413,  ...,  0.1999,  0.1651,  0.0953],\n",
       "         [-0.7413, -0.7936, -0.7064,  ...,  0.0256,  0.0431,  0.0431],\n",
       "         [-0.7064, -0.7761, -0.6715,  ..., -0.0964, -0.0790, -0.0615],\n",
       "         ...,\n",
       "         [-0.2532, -0.2707, -0.2532,  ..., -0.2532, -0.2532, -0.2532],\n",
       "         [-0.3753, -0.3055, -0.2010,  ..., -0.2532, -0.2707, -0.2881],\n",
       "         [-0.3927, -0.3578, -0.2532,  ..., -0.2532, -0.2707, -0.3055]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_both[0,:150528].view((3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_both.narrow(1,0,150528).view((big_both.size(0),3,224,224)).allclose(big_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_both.narrow(1,150528,7)[0].allclose(example_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512])\n",
      "torch.float32\n",
      "torch.Size([4, 7])\n",
      "torch.float32\n",
      "torch.Size([4, 519])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor( np.ones((4,512),dtype=np.float32) )\n",
    "print x.shape\n",
    "print x.dtype\n",
    "\n",
    "m = big_both.narrow(1,150528,7)\n",
    "print m.shape\n",
    "print m.dtype\n",
    "\n",
    "combined = torch.cat((x,m), dim=1)\n",
    "print combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conv1.weight',\n",
       " 'bn1.weight',\n",
       " 'bn1.bias',\n",
       " 'bn1.running_mean',\n",
       " 'bn1.running_var',\n",
       " 'bn1.num_batches_tracked',\n",
       " 'layer1.0.conv1.weight',\n",
       " 'layer1.0.bn1.weight',\n",
       " 'layer1.0.bn1.bias',\n",
       " 'layer1.0.bn1.running_mean',\n",
       " 'layer1.0.bn1.running_var',\n",
       " 'layer1.0.bn1.num_batches_tracked',\n",
       " 'layer1.0.conv2.weight',\n",
       " 'layer1.0.bn2.weight',\n",
       " 'layer1.0.bn2.bias',\n",
       " 'layer1.0.bn2.running_mean',\n",
       " 'layer1.0.bn2.running_var',\n",
       " 'layer1.0.bn2.num_batches_tracked',\n",
       " 'layer1.1.conv1.weight',\n",
       " 'layer1.1.bn1.weight',\n",
       " 'layer1.1.bn1.bias',\n",
       " 'layer1.1.bn1.running_mean',\n",
       " 'layer1.1.bn1.running_var',\n",
       " 'layer1.1.bn1.num_batches_tracked',\n",
       " 'layer1.1.conv2.weight',\n",
       " 'layer1.1.bn2.weight',\n",
       " 'layer1.1.bn2.bias',\n",
       " 'layer1.1.bn2.running_mean',\n",
       " 'layer1.1.bn2.running_var',\n",
       " 'layer1.1.bn2.num_batches_tracked',\n",
       " 'layer2.0.conv1.weight',\n",
       " 'layer2.0.bn1.weight',\n",
       " 'layer2.0.bn1.bias',\n",
       " 'layer2.0.bn1.running_mean',\n",
       " 'layer2.0.bn1.running_var',\n",
       " 'layer2.0.bn1.num_batches_tracked',\n",
       " 'layer2.0.conv2.weight',\n",
       " 'layer2.0.bn2.weight',\n",
       " 'layer2.0.bn2.bias',\n",
       " 'layer2.0.bn2.running_mean',\n",
       " 'layer2.0.bn2.running_var',\n",
       " 'layer2.0.bn2.num_batches_tracked',\n",
       " 'layer2.0.downsample.0.weight',\n",
       " 'layer2.0.downsample.1.weight',\n",
       " 'layer2.0.downsample.1.bias',\n",
       " 'layer2.0.downsample.1.running_mean',\n",
       " 'layer2.0.downsample.1.running_var',\n",
       " 'layer2.0.downsample.1.num_batches_tracked',\n",
       " 'layer2.1.conv1.weight',\n",
       " 'layer2.1.bn1.weight',\n",
       " 'layer2.1.bn1.bias',\n",
       " 'layer2.1.bn1.running_mean',\n",
       " 'layer2.1.bn1.running_var',\n",
       " 'layer2.1.bn1.num_batches_tracked',\n",
       " 'layer2.1.conv2.weight',\n",
       " 'layer2.1.bn2.weight',\n",
       " 'layer2.1.bn2.bias',\n",
       " 'layer2.1.bn2.running_mean',\n",
       " 'layer2.1.bn2.running_var',\n",
       " 'layer2.1.bn2.num_batches_tracked',\n",
       " 'layer3.0.conv1.weight',\n",
       " 'layer3.0.bn1.weight',\n",
       " 'layer3.0.bn1.bias',\n",
       " 'layer3.0.bn1.running_mean',\n",
       " 'layer3.0.bn1.running_var',\n",
       " 'layer3.0.bn1.num_batches_tracked',\n",
       " 'layer3.0.conv2.weight',\n",
       " 'layer3.0.bn2.weight',\n",
       " 'layer3.0.bn2.bias',\n",
       " 'layer3.0.bn2.running_mean',\n",
       " 'layer3.0.bn2.running_var',\n",
       " 'layer3.0.bn2.num_batches_tracked',\n",
       " 'layer3.0.downsample.0.weight',\n",
       " 'layer3.0.downsample.1.weight',\n",
       " 'layer3.0.downsample.1.bias',\n",
       " 'layer3.0.downsample.1.running_mean',\n",
       " 'layer3.0.downsample.1.running_var',\n",
       " 'layer3.0.downsample.1.num_batches_tracked',\n",
       " 'layer3.1.conv1.weight',\n",
       " 'layer3.1.bn1.weight',\n",
       " 'layer3.1.bn1.bias',\n",
       " 'layer3.1.bn1.running_mean',\n",
       " 'layer3.1.bn1.running_var',\n",
       " 'layer3.1.bn1.num_batches_tracked',\n",
       " 'layer3.1.conv2.weight',\n",
       " 'layer3.1.bn2.weight',\n",
       " 'layer3.1.bn2.bias',\n",
       " 'layer3.1.bn2.running_mean',\n",
       " 'layer3.1.bn2.running_var',\n",
       " 'layer3.1.bn2.num_batches_tracked',\n",
       " 'layer4.0.conv1.weight',\n",
       " 'layer4.0.bn1.weight',\n",
       " 'layer4.0.bn1.bias',\n",
       " 'layer4.0.bn1.running_mean',\n",
       " 'layer4.0.bn1.running_var',\n",
       " 'layer4.0.bn1.num_batches_tracked',\n",
       " 'layer4.0.conv2.weight',\n",
       " 'layer4.0.bn2.weight',\n",
       " 'layer4.0.bn2.bias',\n",
       " 'layer4.0.bn2.running_mean',\n",
       " 'layer4.0.bn2.running_var',\n",
       " 'layer4.0.bn2.num_batches_tracked',\n",
       " 'layer4.0.downsample.0.weight',\n",
       " 'layer4.0.downsample.1.weight',\n",
       " 'layer4.0.downsample.1.bias',\n",
       " 'layer4.0.downsample.1.running_mean',\n",
       " 'layer4.0.downsample.1.running_var',\n",
       " 'layer4.0.downsample.1.num_batches_tracked',\n",
       " 'layer4.1.conv1.weight',\n",
       " 'layer4.1.bn1.weight',\n",
       " 'layer4.1.bn1.bias',\n",
       " 'layer4.1.bn1.running_mean',\n",
       " 'layer4.1.bn1.running_var',\n",
       " 'layer4.1.bn1.num_batches_tracked',\n",
       " 'layer4.1.conv2.weight',\n",
       " 'layer4.1.bn2.weight',\n",
       " 'layer4.1.bn2.bias',\n",
       " 'layer4.1.bn2.running_mean',\n",
       " 'layer4.1.bn2.running_var',\n",
       " 'layer4.1.bn2.num_batches_tracked',\n",
       " 'fc.weight',\n",
       " 'fc.bias']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [64, 3, 7, 7], but got 3-dimensional input of size [3, 224, 224] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-a4951c8fbadc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/gweld/anaconda2/envs/sidewalk_pytorch/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/gweld/anaconda2/envs/sidewalk_pytorch/lib/python2.7/site-packages/torch/nn/modules/conv.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [64, 3, 7, 7], but got 3-dimensional input of size [3, 224, 224] instead"
     ]
    }
   ],
   "source": [
    "x = model.conv1(example_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(example_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sidewalk_pytorch]",
   "language": "python",
   "name": "conda-env-sidewalk_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
